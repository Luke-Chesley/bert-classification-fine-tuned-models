{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 10:49:18.015281: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-28 10:49:18.035724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 10:49:18.339268: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 2500)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "\n",
    "def text_normalization(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", string)\n",
    "    string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    string = string.strip()\n",
    "    return string\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"datasets/amazon_reviews_multi-train.parquet\")\n",
    "\n",
    "df = df[[\"review_title\", \"review_body\", \"product_category\"]]\n",
    "\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# df[\"product_class\"] = le.fit_transform(df[\"product_category\"])\n",
    "\n",
    "for col in [\"review_title\", \"review_body\"]:\n",
    "    df[col] = df[col].apply(text_normalization)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"review_title\"] + \" \" + df[\"review_body\"]\n",
    "\n",
    "df = df[[\"text\", \"product_category\"]]\n",
    "\n",
    "SAMPLE_SIZE = 100000\n",
    "\n",
    "df_results = df[0:SAMPLE_SIZE]\n",
    "\n",
    "df = df[0:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(\n",
    "    df[\"product_category\"], columns=[\"product_category\"], prefix=\"\", prefix_sep=\"\"\n",
    ")\n",
    "encoded_df = encoded_df.astype(bool)\n",
    "df = pd.concat([df[\"text\"], encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make into Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apparel</th>\n",
       "      <th>automotive</th>\n",
       "      <th>baby_product</th>\n",
       "      <th>beauty</th>\n",
       "      <th>book</th>\n",
       "      <th>camera</th>\n",
       "      <th>digital_ebook_purchase</th>\n",
       "      <th>digital_video_download</th>\n",
       "      <th>drugstore</th>\n",
       "      <th>electronics</th>\n",
       "      <th>furniture</th>\n",
       "      <th>grocery</th>\n",
       "      <th>home</th>\n",
       "      <th>home_improvement</th>\n",
       "      <th>industrial_supplies</th>\n",
       "      <th>jewelry</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>lawn_and_garden</th>\n",
       "      <th>luggage</th>\n",
       "      <th>musical_instruments</th>\n",
       "      <th>office_product</th>\n",
       "      <th>other</th>\n",
       "      <th>pc</th>\n",
       "      <th>personal_care_appliances</th>\n",
       "      <th>pet_products</th>\n",
       "      <th>shoes</th>\n",
       "      <th>sports</th>\n",
       "      <th>toy</th>\n",
       "      <th>video_games</th>\n",
       "      <th>watch</th>\n",
       "      <th>wireless</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       apparel  automotive  baby_product  beauty   book  camera  \\\n",
       "0        False       False         False   False  False   False   \n",
       "1        False       False         False   False  False   False   \n",
       "2        False       False         False   False  False   False   \n",
       "3        False       False         False   False  False   False   \n",
       "4        False       False         False   False  False   False   \n",
       "...        ...         ...           ...     ...    ...     ...   \n",
       "99995    False       False         False    True  False   False   \n",
       "99996    False       False         False   False  False   False   \n",
       "99997    False       False         False   False  False   False   \n",
       "99998    False       False         False   False  False   False   \n",
       "99999    False       False         False   False  False   False   \n",
       "\n",
       "       digital_ebook_purchase  digital_video_download  drugstore  electronics  \\\n",
       "0                       False                   False      False        False   \n",
       "1                       False                   False      False        False   \n",
       "2                       False                   False      False        False   \n",
       "3                       False                   False      False        False   \n",
       "4                       False                   False      False        False   \n",
       "...                       ...                     ...        ...          ...   \n",
       "99995                   False                   False      False        False   \n",
       "99996                   False                   False      False        False   \n",
       "99997                   False                   False      False        False   \n",
       "99998                   False                   False      False        False   \n",
       "99999                   False                   False      False        False   \n",
       "\n",
       "       furniture  grocery   home  home_improvement  industrial_supplies  \\\n",
       "0           True    False  False             False                False   \n",
       "1          False    False  False              True                False   \n",
       "2          False    False   True             False                False   \n",
       "3          False    False  False             False                False   \n",
       "4          False    False  False             False                False   \n",
       "...          ...      ...    ...               ...                  ...   \n",
       "99995      False    False  False             False                False   \n",
       "99996      False    False  False             False                False   \n",
       "99997      False    False  False             False                False   \n",
       "99998      False    False   True             False                False   \n",
       "99999      False    False  False             False                False   \n",
       "\n",
       "       jewelry  kitchen  lawn_and_garden  luggage  musical_instruments  \\\n",
       "0        False    False            False    False                False   \n",
       "1        False    False            False    False                False   \n",
       "2        False    False            False    False                False   \n",
       "3        False    False            False    False                False   \n",
       "4        False    False            False    False                False   \n",
       "...        ...      ...              ...      ...                  ...   \n",
       "99995    False    False            False    False                False   \n",
       "99996    False    False            False    False                False   \n",
       "99997    False    False            False    False                False   \n",
       "99998    False    False            False    False                False   \n",
       "99999    False    False            False    False                False   \n",
       "\n",
       "       office_product  other     pc  personal_care_appliances  pet_products  \\\n",
       "0               False  False  False                     False         False   \n",
       "1               False  False  False                     False         False   \n",
       "2               False  False  False                     False         False   \n",
       "3               False  False  False                     False         False   \n",
       "4               False  False   True                     False         False   \n",
       "...               ...    ...    ...                       ...           ...   \n",
       "99995           False  False  False                     False         False   \n",
       "99996           False   True  False                     False         False   \n",
       "99997           False  False  False                     False         False   \n",
       "99998           False  False  False                     False         False   \n",
       "99999           False  False  False                     False         False   \n",
       "\n",
       "       shoes  sports    toy  video_games  watch  wireless  \n",
       "0      False   False  False        False  False     False  \n",
       "1      False   False  False        False  False     False  \n",
       "2      False   False  False        False  False     False  \n",
       "3      False   False  False        False  False      True  \n",
       "4      False   False  False        False  False     False  \n",
       "...      ...     ...    ...          ...    ...       ...  \n",
       "99995  False   False  False        False  False     False  \n",
       "99996  False   False  False        False  False     False  \n",
       "99997  False   False  False        False  False      True  \n",
       "99998  False   False  False        False  False     False  \n",
       "99999  False   False   True        False  False     False  \n",
       "\n",
       "[100000 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the split DataFrames into Datasets\n",
    "train = Dataset.from_pandas(train_df, split=\"train\")\n",
    "valid = Dataset.from_pandas(valid_df, split=\"validation\")\n",
    "test = Dataset.from_pandas(test_df, split=\"test\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train, \"validation\": valid, \"test\": test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    label\n",
    "    for label in dataset[\"train\"].features.keys()\n",
    "    if label not in [\"text\", \"__index_level_0__\"]\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"label_dicts/id2label.pkl\", \"wb\") as f:\n",
    "#  pickle.dump(id2label, f)\n",
    "\n",
    "# with open(\"label_dicts/label2id.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(label2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2336ba2a179c42f99a2db84dac6d88c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c577314c41d04c22863c331be18036c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f1014b31bb4f70b5c2a471a3ba86eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = encoded_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] two stars had it for two weeks and after removing cards and id from slot it crack very thin material [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {\"f1\": f1_micro_average, \"roc_auc\": roc_auc, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][0][\"labels\"].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2048,  3340,  2018,  2009,  2005,  2048,  3134,  1998,  2044,\n",
       "         9268,  5329,  1998,  8909,  2013, 10453,  2009,  8579,  2200,  4857,\n",
       "         3430,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m      2\u001b[0m     input_ids\u001b[39m=\u001b[39;49mencoded_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m      3\u001b[0m     labels\u001b[39m=\u001b[39;49mencoded_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoded_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1563\u001b[0m     input_ids,\n\u001b[1;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1572\u001b[0m )\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1015\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1018\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1019\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:232\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    229\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    233\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    235\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "outputs = model(\n",
    "    input_ids=encoded_dataset[\"train\"][\"input_ids\"][0].unsqueeze(0),\n",
    "    labels=encoded_dataset[\"train\"][0][\"labels\"].unsqueeze(0),\n",
    "    attention_mask=encoded_dataset[\"train\"][0][\"attention_mask\"].unsqueeze(0),\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb266072d83457385764cffbc0c1f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1814\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfd73ee2b784ff5a8fead8b0437ae99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0825805515050888,\n",
       " 'eval_f1': 0.522999472209817,\n",
       " 'eval_roc_auc': 0.7111555555555555,\n",
       " 'eval_accuracy': 0.4282,\n",
       " 'eval_runtime': 88.8685,\n",
       " 'eval_samples_per_second': 168.789,\n",
       " 'eval_steps_per_second': 21.099,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.1\n",
    "\n",
    "\n",
    "def inference(text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k, v in encoding.items()}\n",
    "\n",
    "    outputs = trainer.model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= CONFIDENCE_THRESHOLD)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [\n",
    "        id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "    ]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [furniture, home]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"text\"][0:1].apply(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_TO_EVALUATE = 100\n",
    "\n",
    "df_results[\"predicted_class\"] = df_results[\"text\"][0:ROWS_TO_EVALUATE].apply(inference)\n",
    "df_results[\"correct\"] = df_results[0:ROWS_TO_EVALUATE].apply(\n",
    "    lambda row: int(row[\"product_category\"] in row[\"predicted_class\"]), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def correct_discount(row):\n",
    "    num_guesses = len(row[\"predicted_class\"])\n",
    "\n",
    "    if num_guesses > 1 and row[\"correct\"] == 1:\n",
    "        score = (0.85) ** num_guesses\n",
    "\n",
    "        return score\n",
    "    else:\n",
    "        return row[\"correct\"]\n",
    "\n",
    "\n",
    "df_results[\"correct_w_discount\"] = df_results[0:ROWS_TO_EVALUATE].apply(\n",
    "    correct_discount, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"correct\"].sum() / ROWS_TO_EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65068617328125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[\"correct_w_discount\"].sum() / ROWS_TO_EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product_category</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>correct</th>\n",
       "      <th>correct_w_discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ll spend twice the amount of time boxing up the whole useless thing and send it back with a 1 star review arrived broken manufacturer defect two of the legs of the base were not completely formed so there was no way to insert the casters i unpackaged the entire chair and hardware before noticing this so i ll spend twice the amount of time boxing up the whole useless thing and send it back with a 1 star review of part of a chair i never got to sit in i will go so far as to include a picture...</td>\n",
       "      <td>furniture</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not use able the cabinet dot were all detached from backing got me</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the product is junk i received my first order of this product and it was broke so i ordered it again the second one was broke in more places than the first i can t blame the shipping process as it s shrink wrapped and boxed</td>\n",
       "      <td>home</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fucking waste of money this product is a piece of shit do not buy doesn t work and then i try to call for customer support it won t take my number fucking rip off</td>\n",
       "      <td>wireless</td>\n",
       "      <td>[baby_product, camera, industrial_supplies]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bubble went through 3 in one day doesn t fit correct and couldn t get bubbles out better without</td>\n",
       "      <td>pc</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>terrible don t waste your money terrible don t waste your money it s a jap knockoff of realtor and it totally sucks could connect to my router but after that no dice no web access brilliant scam well done</td>\n",
       "      <td>pc</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>cute but definitely for a small child my 6 cute but definitely for a small child my 6 year old grand daughter didn t want it because it wasn t a real violin</td>\n",
       "      <td>toy</td>\n",
       "      <td>[baby_product, industrial_supplies, other, wireless]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sucks the product worked for maybe five minutes then it stopped and my iphone said is not compatible with iphone</td>\n",
       "      <td>wireless</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>buyer beware phone does not work and seller asked for pictures instead of replacing defective unit phone does not work when plugged in to an active land line no dial tone seller asked for pictures</td>\n",
       "      <td>office_product</td>\n",
       "      <td>[automotive, baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>would not recommend for use with essential oils i bought these bottles with the intention of mixing essential oils in them the spray hardly works and they leak frequently would not recommend for use with essential oils</td>\n",
       "      <td>beauty</td>\n",
       "      <td>[baby_product, industrial_supplies, other]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "0   i ll spend twice the amount of time boxing up the whole useless thing and send it back with a 1 star review arrived broken manufacturer defect two of the legs of the base were not completely formed so there was no way to insert the casters i unpackaged the entire chair and hardware before noticing this so i ll spend twice the amount of time boxing up the whole useless thing and send it back with a 1 star review of part of a chair i never got to sit in i will go so far as to include a picture...   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                    not use able the cabinet dot were all detached from backing got me   \n",
       "2                                                                                                                                                                                                                                                                                       the product is junk i received my first order of this product and it was broke so i ordered it again the second one was broke in more places than the first i can t blame the shipping process as it s shrink wrapped and boxed   \n",
       "3                                                                                                                                                                                                                                                                                                                                                    fucking waste of money this product is a piece of shit do not buy doesn t work and then i try to call for customer support it won t take my number fucking rip off   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                      bubble went through 3 in one day doesn t fit correct and couldn t get bubbles out better without   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...   \n",
       "95                                                                                                                                                                                                                                                                                                         terrible don t waste your money terrible don t waste your money it s a jap knockoff of realtor and it totally sucks could connect to my router but after that no dice no web access brilliant scam well done   \n",
       "96                                                                                                                                                                                                                                                                                                                                                         cute but definitely for a small child my 6 cute but definitely for a small child my 6 year old grand daughter didn t want it because it wasn t a real violin   \n",
       "97                                                                                                                                                                                                                                                                                                                                                                                                     sucks the product worked for maybe five minutes then it stopped and my iphone said is not compatible with iphone   \n",
       "98                                                                                                                                                                                                                                                                                                                 buyer beware phone does not work and seller asked for pictures instead of replacing defective unit phone does not work when plugged in to an active land line no dial tone seller asked for pictures   \n",
       "99                                                                                                                                                                                                                                                                                           would not recommend for use with essential oils i bought these bottles with the intention of mixing essential oils in them the spray hardly works and they leak frequently would not recommend for use with essential oils   \n",
       "\n",
       "    product_category                                         predicted_class  \\\n",
       "0          furniture              [baby_product, industrial_supplies, other]   \n",
       "1   home_improvement              [baby_product, industrial_supplies, other]   \n",
       "2               home              [baby_product, industrial_supplies, other]   \n",
       "3           wireless             [baby_product, camera, industrial_supplies]   \n",
       "4                 pc              [baby_product, industrial_supplies, other]   \n",
       "..               ...                                                     ...   \n",
       "95                pc              [baby_product, industrial_supplies, other]   \n",
       "96               toy    [baby_product, industrial_supplies, other, wireless]   \n",
       "97          wireless              [baby_product, industrial_supplies, other]   \n",
       "98    office_product  [automotive, baby_product, industrial_supplies, other]   \n",
       "99            beauty              [baby_product, industrial_supplies, other]   \n",
       "\n",
       "    correct  correct_w_discount  \n",
       "0         0                 0.0  \n",
       "1         0                 0.0  \n",
       "2         0                 0.0  \n",
       "3         0                 0.0  \n",
       "4         0                 0.0  \n",
       "..      ...                 ...  \n",
       "95        0                 0.0  \n",
       "96        0                 0.0  \n",
       "97        0                 0.0  \n",
       "98        0                 0.0  \n",
       "99        0                 0.0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save model (already saved)\n",
    "#trainer.save_model(\"bert-finetuned-amazon-reviews-multi-train-100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-finetuned-amazon-reviews-multi-train-100k\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = Trainer(model=model1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love my new phone. It's really great! The screen is big and bright. The battery lasts a long time\"\n",
    "\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer1.model.device) for k, v in encoding.items()}\n",
    "\n",
    "outputs = trainer1.model(**encoding)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= CONFIDENCE_THRESHOLD)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [\n",
    "    id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"datasets/amazon_reviews_multi-test.parquet\")\n",
    "\n",
    "df = df[[\"review_title\", \"review_body\", \"product_category\"]]\n",
    "\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# df[\"product_class\"] = le.fit_transform(df[\"product_category\"])\n",
    "\n",
    "for col in [\"review_title\", \"review_body\"]:\n",
    "    df[col] = df[col].apply(text_normalization)\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"review_title\"] + \" \" + df[\"review_body\"]\n",
    "\n",
    "df_test = df[[\"text\", \"product_category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-finetuned-amazon-reviews-multi-train-100k\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "trainer1 = Trainer(model=model1, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.1\n",
    "\n",
    "\n",
    "def inference(text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer1.model.device) for k, v in encoding.items()}\n",
    "\n",
    "    outputs = trainer1.model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "    predictions[np.where(probs >= CONFIDENCE_THRESHOLD)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [\n",
    "        id2label[idx] for idx, label in enumerate(predictions) if label == 1.0\n",
    "    ]\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1296438/2932677383.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"predicted_class\"] = df_test[\"text\"][0:ROWS_TO_EVALUATE].apply(inference)\n"
     ]
    }
   ],
   "source": [
    "ROWS_TO_EVALUATE = len(df_test)\n",
    "\n",
    "df_test[\"predicted_class\"] = df_test[\"text\"][0:ROWS_TO_EVALUATE].apply(inference)\n",
    "df_test[\"correct\"] = df_test[0:ROWS_TO_EVALUATE].apply(\n",
    "    lambda row: int(row[\"product_category\"] in row[\"predicted_class\"]), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "def correct_discount(row, muli_cat_guess_penalty=0.85):\n",
    "    num_guesses = len(row[\"predicted_class\"])\n",
    "\n",
    "    if num_guesses > 1 and row[\"correct\"] == 1:\n",
    "        score = (muli_cat_guess_penalty) ** num_guesses\n",
    "\n",
    "        return score\n",
    "    else:\n",
    "        return row[\"correct\"]\n",
    "\n",
    "\n",
    "df_test[\"correct_w_discount\"] = df_test[0:ROWS_TO_EVALUATE].apply(\n",
    "    correct_discount, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of predictions that include correct class: 95.4%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Percent of predictions that include correct class: {round((df_test.correct.sum() / ROWS_TO_EVALUATE)*100, 2)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product_category</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>correct</th>\n",
       "      <th>correct_w_discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>don t waste your time these are awful they are see through the fabric feels like tablecloth and they fit like children s clothing customer service did seem to be nice though but i regret missing my return date for these i wouldn t even donate them because the quality is so poor</td>\n",
       "      <td>apparel</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one star i bought 4 and none of them worked yes i used new batteries</td>\n",
       "      <td>other</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>totally useless on first use it didn t heat up and now it doesn t work at all</td>\n",
       "      <td>other</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gold filled earrings you want an honest answer i just returned from ups where i returned the farce of an earring set to amazon it did not look like what i saw on amazon only a baby would be able to wear the size of the earring they were so small the size of a pin head i at first thought amazon had forgotten to enclose them in the bag i didn t bother to take them out of the bag and you can have them back will never order another thing from your company a disgrace honest enough for you grandma</td>\n",
       "      <td>jewelry</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>poor container the glue works fine but the container is impossible to work with the cap doesn t come off without plyers and then won t go back on without a violent abrupt force involving both hands and a solid object desk drawer this happened even though i was careful to not gum up the lid or tapering snout</td>\n",
       "      <td>industrial_supplies</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>very sleek it s still early but so far the scales seems like a good product it is very modern looking my wife got back from the dr s office and said their scale read 3 pounds lighter than ours so time to shop ours was old so maybe lost some degree of accuracy this inevifit seems to be accurate time will tell we like it hard to give products a 5 when they are still in the testing phase and there is always room for improvement i don t really like to have to use 4 aaa batteries but i ll get ove...</td>\n",
       "      <td>drugstore</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>face mask a few times now and it works great it also had a pleasant smell will buy it i ve used this face mask a few times now and it works great it also had a pleasant smell will buy it again</td>\n",
       "      <td>beauty</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>five stars great price good quality</td>\n",
       "      <td>home</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2016 f150 xlt fits my 2016 f150 xlt great now i dont have to listen to it hitting side to side while driving anymore</td>\n",
       "      <td>automotive</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>perfect just what i needed perfect for western theme party</td>\n",
       "      <td>apparel</td>\n",
       "      <td>[apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "0                                                                                                                                                                                                                                  don t waste your time these are awful they are see through the fabric feels like tablecloth and they fit like children s clothing customer service did seem to be nice though but i regret missing my return date for these i wouldn t even donate them because the quality is so poor   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                    one star i bought 4 and none of them worked yes i used new batteries   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                           totally useless on first use it didn t heat up and now it doesn t work at all   \n",
       "3        gold filled earrings you want an honest answer i just returned from ups where i returned the farce of an earring set to amazon it did not look like what i saw on amazon only a baby would be able to wear the size of the earring they were so small the size of a pin head i at first thought amazon had forgotten to enclose them in the bag i didn t bother to take them out of the bag and you can have them back will never order another thing from your company a disgrace honest enough for you grandma   \n",
       "4                                                                                                                                                                                                    poor container the glue works fine but the container is impossible to work with the cap doesn t come off without plyers and then won t go back on without a violent abrupt force involving both hands and a solid object desk drawer this happened even though i was careful to not gum up the lid or tapering snout   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n",
       "4995  very sleek it s still early but so far the scales seems like a good product it is very modern looking my wife got back from the dr s office and said their scale read 3 pounds lighter than ours so time to shop ours was old so maybe lost some degree of accuracy this inevifit seems to be accurate time will tell we like it hard to give products a 5 when they are still in the testing phase and there is always room for improvement i don t really like to have to use 4 aaa batteries but i ll get ove...   \n",
       "4996                                                                                                                                                                                                                                                                                                                     face mask a few times now and it works great it also had a pleasant smell will buy it i ve used this face mask a few times now and it works great it also had a pleasant smell will buy it again   \n",
       "4997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  five stars great price good quality   \n",
       "4998                                                                                                                                                                                                                                                                                                                                                                                                 2016 f150 xlt fits my 2016 f150 xlt great now i dont have to listen to it hitting side to side while driving anymore   \n",
       "4999                                                                                                                                                                                                                                                                                                                                                                                                                                                           perfect just what i needed perfect for western theme party   \n",
       "\n",
       "         product_category  \\\n",
       "0                 apparel   \n",
       "1                   other   \n",
       "2                   other   \n",
       "3                 jewelry   \n",
       "4     industrial_supplies   \n",
       "...                   ...   \n",
       "4995            drugstore   \n",
       "4996               beauty   \n",
       "4997                 home   \n",
       "4998           automotive   \n",
       "4999              apparel   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                        predicted_class  \\\n",
       "0     [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "1     [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "2     [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "3     [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "4     [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "...                                                                                                                                                                                                                                                                                                 ...   \n",
       "4995  [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "4996  [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "4997  [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "4998  [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "4999  [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, office_product, other, pc, pet_products, sports, toy, video_games, watch, wireless]   \n",
       "\n",
       "      correct  correct_w_discount  \n",
       "0           1            0.014618  \n",
       "1           1            0.014618  \n",
       "2           1            0.014618  \n",
       "3           1            0.014618  \n",
       "4           1            0.014618  \n",
       "...       ...                 ...  \n",
       "4995        1            0.014618  \n",
       "4996        1            0.014618  \n",
       "4997        1            0.014618  \n",
       "4998        1            0.014618  \n",
       "4999        1            0.014618  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
