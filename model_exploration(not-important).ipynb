{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 09:47:48.605953: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-09 09:47:48.757426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 09:47:49.280075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run local_functions.py\n",
    "from local_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 2500)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size\n",
    "    ):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        seq_length = input_shape[1]\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
    "\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = hidden_size // num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = (\n",
    "            context_layer.permute(0, 2, 1, 3).contiguous().view(hidden_states.size())\n",
    "        )\n",
    "        return context_layer\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        intermediate_output = self.dense(hidden_states)\n",
    "        intermediate_output = self.intermediate_act_fn(intermediate_output)\n",
    "        return intermediate_output\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size, hidden_size):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, intermediate_output, hidden_states):\n",
    "        hidden_states = self.dense(intermediate_output)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, intermediate_size):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertSelfAttention(hidden_size, num_attention_heads)\n",
    "        self.intermediate = BertIntermediate(hidden_size, intermediate_size)\n",
    "        self.output = BertOutput(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_hidden_layers, hidden_size, num_attention_heads, intermediate_size\n",
    "    ):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        layer = BertLayer(hidden_size, num_attention_heads, intermediate_size)\n",
    "        self.layer = nn.ModuleList([layer for _ in range(num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        return all_hidden_states, all_attentions\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class MyBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyBertForSequenceClassification, self).__init__()\n",
    "\n",
    "        self.embeddings = BertEmbeddings(30522, 768, 512, 2)\n",
    "        self.encoder = BertEncoder(12, 768, 12, 3072)\n",
    "        self.pooler = BertPooler(768)\n",
    "        self.classifier = nn.Sequential(nn.Linear(768, num_classes))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        if id2label is not None and label2id is not None:\n",
    "            self.id2label = id2label\n",
    "            self.label2id = label2id\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        all_hidden_states, _ = self.encoder(embedding_output, attention_mask)\n",
    "        sequence_output = all_hidden_states[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyBertForSequenceClassification(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELU(approximate='none')\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/wiki_movie_plots_deduped.csv\")\n",
    "df = df[[\"Plot\", \"Genre\"]]\n",
    "df = df.loc[df.Genre != \"unknown\"].reset_index(drop=True)\n",
    "df[\"text\"] = df[\"Plot\"].apply(text_normalization_2)\n",
    "df.drop(\"Plot\", axis=1, inplace=True)\n",
    "df = df[[\"text\", \"Genre\"]]\n",
    "# retain rows with top 100 genres\n",
    "top_100_genres = df[\"Genre\"].value_counts().head(100).index.tolist()\n",
    "df = df.loc[df[\"Genre\"].isin(top_100_genres)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, testing_df = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(\n",
    "    training_df[\"Genre\"], columns=[\"Genre\"], prefix=\"\", prefix_sep=\"\"\n",
    ")\n",
    "encoded_df = encoded_df.astype(bool)\n",
    "encoded_df_con = pd.concat([training_df[\"text\"], encoded_df], axis=1)\n",
    "\n",
    "train_df, temp_df = train_test_split(encoded_df_con, test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert the split DataFrames into Datasets\n",
    "train = Dataset.from_pandas(train_df, split=\"train\")\n",
    "valid = Dataset.from_pandas(valid_df, split=\"validation\")\n",
    "test = Dataset.from_pandas(test_df, split=\"test\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": train, \"validation\": valid, \"test\": test})\n",
    "\n",
    "labels = [\n",
    "    label\n",
    "    for label in dataset[\"train\"].features.keys()\n",
    "    if label not in [\"text\", \"__index_level_0__\"]\n",
    "]\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0d3088a7d54739812b7618f93fd16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12770 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f42a56c264673ab938093b59258ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e836d14976574eecb0c9f125ace29760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # take a batch of texts\n",
    "    text = examples[\"text\"]\n",
    "    # encode them\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    # create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "    # fill numpy array\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    preprocess_data, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyBertForSequenceClassification.__init__() got an unexpected keyword argument 'id2label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/luke/projects/jupyterlab/Notebooks/nlp classification/model_exploration(not-important).ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/nlp%20classification/model_exploration%28not-important%29.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_1 \u001b[39m=\u001b[39m MyBertForSequenceClassification(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/nlp%20classification/model_exploration%28not-important%29.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(labels),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/nlp%20classification/model_exploration%28not-important%29.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     id2label\u001b[39m=\u001b[39;49mid2label,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/nlp%20classification/model_exploration%28not-important%29.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     label2id\u001b[39m=\u001b[39;49mlabel2id,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luke/projects/jupyterlab/Notebooks/nlp%20classification/model_exploration%28not-important%29.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: MyBertForSequenceClassification.__init__() got an unexpected keyword argument 'id2label'"
     ]
    }
   ],
   "source": [
    "model_1 = MyBertForSequenceClassification(\n",
    "    num_classes=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
